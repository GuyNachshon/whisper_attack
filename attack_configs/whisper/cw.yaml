# Root path configuration
root: !PLACEHOLDER  # Will be overridden by script

# General settings
seed: 1002  # Random seed for reproducibility
__set_seed: !apply:torch.manual_seed [!ref <seed>]

# Device settings
device: "cuda:0"
precision: 32  # Use FP32 for stability
fp16: False    # Disable FP16 for now
jit: False
auto_mix_prec: False  # Disable mixed precision
smoothing: False      # Disable smoothing that was causing CUDA errors
cuda_deterministic: True
cudnn_benchmark: False

# Model initialization settings
model_init:
  device: !ref <device>
  dtype: !name:torch.float32
  download_root: null
  in_memory: True
  with_grad: True

# Data files and paths
data_folder: !ref <root>/LibriSpeech
csv_folder: !ref <data_folder>/csv
test_splits: ["test-clean"]
skip_prep: False

# Model settings
model_label: "tiny"
download_data: True
model_name: !ref whisper-<model_label>
target_brain_class: !name:sb_whisper_binding.WhisperASR
target_brain_hparams_file: !ref model_configs/<model_label>.yaml

# Attack parameters
eps: 0.1
nb_iter: 1000
const: 100
lr: 0.01
confidence: 0.0
attack_name: cw
save_audio: True
load_audio: True

# Training settings
batch_size: 1
avoid_if_longer_than: 14.0
sorting: random
sample_rate: 16000

# Output settings
output_folder: !ref <root>/attacks/<attack_name>/<model_name>/<seed>
wer_file: !ref <output_folder>/wer.txt
save_folder: !ref <output_folder>
log: !ref <output_folder>/log.txt
save_audio_path: !ref <output_folder>/save

# Tokenizer settings
tokenizer_name: multilingual
tokenizer_builder: !name:whisper.tokenizer.get_tokenizer

# Dataset settings
dataset_prepare_fct: !name:robust_speech.data.librispeech.prepare_librispeech
dataio_prepare_fct: !name:robust_speech.data.dataio.dataio_prepare

# Test settings
test_dataloader_opts:
    batch_size: 1

logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <log> 